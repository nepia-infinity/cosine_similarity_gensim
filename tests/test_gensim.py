import pandas as pd
import numpy as np
import spacy
from gensim.models.doc2vec import Doc2Vec, TaggedDocument
from typing import List, Dict, Any

# GiNZAãƒ¢ãƒ‡ãƒ«ã®ãƒ­ãƒ¼ãƒ‰ã¯ä¸€åº¦ã ã‘è¡Œã†
try:
    # å®Ÿè¡Œç’°å¢ƒã«åˆã‚ã›ã¦ 'ja_ginza' ã¾ãŸã¯ 'ja_ginza_electra' ãªã©ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
    nlp = spacy.load("ja_ginza_electra")
except OSError as e:
    print(f"ã‚¨ãƒ©ãƒ¼: GiNZAãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚äº‹å‰ã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚è©³ç´°: {e}")
    nlp = None

# Doc2Vecã®å­¦ç¿’ã«ä½¿ç”¨ã™ã‚‹å“è©
target_pos = ['NOUN', 'VERB', 'ADJ', 'ADV']

# ----------------------------------------------------------------------
# ## 1. ãƒ‡ãƒ¼ã‚¿æº–å‚™ã¨å½¢æ…‹ç´ è§£æ (Data Preparation)
# ----------------------------------------------------------------------

def load_data(file_path: str) -> pd.DataFrame:
    """CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‡ãƒ¼ã‚¿ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’è¿”ã™"""
    try:
        data = pd.read_csv(file_path)
        print(f"ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸã€‚ç·æ–‡æ›¸æ•°: {len(data)}")
        return data
    except FileNotFoundError:
        print(f"ã‚¨ãƒ©ãƒ¼: '{file_path}'ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return pd.DataFrame()

def tokenize_data(data: pd.DataFrame) -> List[TaggedDocument]:
    """GiNZAã‚’ä½¿ç”¨ã—ã¦æ–‡æ›¸ã‚’å½¢æ…‹ç´ è§£æã—ã€TaggedDocumentãƒªã‚¹ãƒˆã‚’ç”Ÿæˆã™ã‚‹"""
    if nlp is None:
        return []
        
    print("å½¢æ…‹ç´ è§£æã¨TaggedDocumentã®ä½œæˆã‚’é–‹å§‹...")

    def tokenize_ginza(text):
        """æŒ‡å®šå“è©ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ã¿ã‚’æŠ½å‡º"""
        doc = nlp(text)
        tokens = [
            token.text
            for token in doc
            if token.pos_ in target_pos and not token.is_stop
        ]
        return tokens

    tagged_data = [
        TaggedDocument(
            words=tokenize_ginza(row['text']),
            tags=[str(row['row_index'])]
        )
        for _, row in data.iterrows()
    ]
    print("TaggedDocumentã®ä½œæˆãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    return tagged_data

# ----------------------------------------------------------------------
# ## 2. ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ (Model Training)
# ----------------------------------------------------------------------

def train_doc2vec_model(tagged_data: List[TaggedDocument]) -> Doc2Vec:
    """TaggedDocumentãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦Doc2Vecãƒ¢ãƒ‡ãƒ«ã‚’å­¦ç¿’ã•ã›ã‚‹"""
    
    model = Doc2Vec(
        vector_size=75,       
        window=5,             
        min_count=1, # 1å›ã§ã‚‚ç™»å ´ã—ãŸå˜èªã‚’å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã«åŠ ãˆã‚‹       
        workers=4,            
        epochs=40             
    )

    model.build_vocab(tagged_data)
    print("Doc2Vecãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™...")
    model.train(tagged_data, total_examples=model.corpus_count, epochs=model.epochs)
    print("Doc2Vecãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
    
    return model

# ----------------------------------------------------------------------
# ## 3. é¡ä¼¼åº¦è¨ˆç®—ã¨çµæœå‡ºåŠ› (Similarity Calculation and Output)
# ----------------------------------------------------------------------

def tokenize_data_single(text: str) -> List[str]:
    """å˜ä¸€ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’å½¢æ…‹ç´ è§£æã™ã‚‹ (ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°)"""
    if nlp is None:
        return []
    
    doc = nlp(text)
    tokens = [
        token.text
        for token in doc
        if token.pos_ in target_pos and not token.is_stop
    ]
    return tokens

def calculate_similarity_and_rank(model: Doc2Vec, data: pd.DataFrame, query: str) -> List[Dict[str, Any]]:
    """
    ã‚¯ã‚¨ãƒªæ–‡ã¨å­¦ç¿’æ¸ˆã¿æ–‡æ›¸ã¨ã®é¡ä¼¼åº¦ã‚’è¨ˆç®—ã—ã€ä¸Šä½5ä»¶ã®çµæœã‚’é…åˆ—ã§è¿”ã™
    """
    if nlp is None:
        return []

    query_tokens = tokenize_data_single(query)
    query_vector = model.infer_vector(query_tokens, epochs=20)

    results = []
    for _, row in data.iterrows():
        doc_id = str(row['row_index'])
        doc_vector = model.dv.get_vector(doc_id)
        
        similarity = np.dot(query_vector, doc_vector) / (
            np.linalg.norm(query_vector) * np.linalg.norm(doc_vector)
        )
        
        results.append({
            'row_index': int(doc_id),
            'document': row['text'],
            'similarity': similarity
        })

    sorted_results = sorted(results, key=lambda x: x['similarity'], reverse=True)
    return sorted_results[:5]

def print_results(top_5_results_array: List[Dict[str, Any]]):
    """ä¸Šä½5ä»¶ã®çµæœã‚’æ•´å½¢ã—ã¦ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ã«å‡ºåŠ›ã™ã‚‹"""
    print("\n-------------------------------------------")
    print("âœ… ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã«ã‚ˆã‚‹é¡ä¼¼æ–‡æ›¸ãƒ©ãƒ³ã‚­ãƒ³ã‚° (ä¸Šä½5ä»¶)")
    print("-------------------------------------------")

    print("{:<10} {:<30} {:>10}".format("Row_Index", "Document", "Similarity"))
    print("-" * 52)
    for result in top_5_results_array:
        display_text = result['document']
        if len(display_text) > 25:
            display_text = display_text[:24] + "..."
            
        print("{:<10} {:<30} {:>10.6f}".format(
            result['row_index'],
            display_text,
            result['similarity']
        ))

    print("\n--- ä¸Šä½5ä»¶ã®çµæœé…åˆ—ï¼ˆãƒªã‚¹ãƒˆï¼‰ ---")
    print(top_5_results_array)

# ----------------------------------------------------------------------
# ## 4. ãƒ¡ã‚¤ãƒ³å‡¦ç†ã®å®Ÿè¡Œ
# ----------------------------------------------------------------------

if __name__ == "__main__":
    if nlp is None:
        exit()
        
    query_text = "ãƒ—ãƒ­ã‚°ãƒ©ãƒŸãƒ³ã‚°ã¯ã¨ã¦ã‚‚é¢ç™½ã„ã§ã™ã€‚PythonãŒå¥½ãã€‚"
    print(f"ğŸ”‘å…¥åŠ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ: '{query_text}'")
    data_file = r'C:\Users\nepia\Desktop\gensim\csv\test_data.csv'
    
    data_df = load_data(data_file) 
    if data_df.empty:
        exit()
        
    tagged_data_list = tokenize_data(data_df)
    doc2vec_model = train_doc2vec_model(tagged_data_list)
    top_5_ranking = calculate_similarity_and_rank(doc2vec_model, data_df, query_text)
    print_results(top_5_ranking)
