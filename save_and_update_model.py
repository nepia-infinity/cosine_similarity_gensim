from gensim.models.doc2vec import Doc2Vec, TaggedDocument
import pandas as pd
import spacy
import pytz
from datetime import datetime
from common import get_current_japanese_timestamp
import os

# 1. GiNZAãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰
try:
    # å¿…è¦ã«å¿œã˜ã¦ "ja_ginza" ã«å¤‰ãˆã¦ãã ã•ã„
    nlp = spacy.load("ja_ginza_electra")
except OSError as e:
    print("GiNZAãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚å…ˆã«ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„ã€‚")
    print(e)
    nlp = None

# Doc2Vecã§ä½¿ã„ãŸã„å“è©
TARGET_POS = ["NOUN", "VERB", "ADJ", "ADV"]
BASE_DIR = r"C:\Users\nepia\Desktop\gensim\models"


def load_data(file_path: str) -> pd.DataFrame:
    """CSVã‚’èª­ã¿è¾¼ã‚“ã§DataFrameã‚’è¿”ã™"""
    try:
        df = pd.read_csv(file_path)
        print(f"âœ… CSVã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ: {file_path}  ç·è¡Œæ•°: {len(df)}")
        return df
    except FileNotFoundError:
        print(f"âŒ CSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {file_path}")
        return pd.DataFrame()


def to_tagged_docs(df: pd.DataFrame):
    """DataFrame -> TaggedDocument ã®ãƒªã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹"""
    if nlp is None:
        return []

    tagged_docs = []

    for _, row in df.iterrows():
        text = str(row["text"])
        row_id = str(row["row_index"]) if "row_index" in df.columns else str(_)

        doc = nlp(text)
        tokens = [
            token.text
            for token in doc
            if token.pos_ in TARGET_POS and not token.is_stop
        ]

        tagged_docs.append(TaggedDocument(words=tokens, tags=[row_id]))

    print(f"âœ… TaggedDocument ã‚’ {len(tagged_docs)} ä»¶ ä½œæˆã—ã¾ã—ãŸã€‚")
    return tagged_docs


def save_and_update_model(tagged_data, base_dir="models") -> Doc2Vec:
    """æ—¥æœ¬æ™‚é–“ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«åã§ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜"""
    # ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ä½œæˆ
    os.makedirs(BASE_DIR, exist_ok=True)

    # æ—¥æœ¬æ™‚é–“ã®ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’ç”Ÿæˆã™ã‚‹
    timestamp = get_current_japanese_timestamp()

    # ãƒ•ã‚¡ã‚¤ãƒ«åã‚’ä½œæˆ
    model_path = os.path.join(BASE_DIR, f"doc2vec_model_{timestamp}.model")
    print(f"ãƒ¢ãƒ‡ãƒ«ã‚’æ–°è¦ä½œæˆã—ã¾ã™: {model_path}")
    
    model = Doc2Vec(
        vector_size=75,
        window=5,
        min_count=1,
        workers=4,
        epochs=40,
    )
    model.build_vocab(tagged_data)
    model.train(tagged_data, total_examples=len(tagged_data), epochs=model.epochs)
    model.save(model_path)

    print(f"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {model_path}")
    return model


if __name__ == "__main__":
    # ã‚ãªãŸã®ãƒ‘ã‚¹
    data_file = r"C:\Users\nepia\Desktop\gensim\csv\test_data.csv"
    model_path = "doc2vec_model.model"

    if nlp is None:
        # å½¢æ…‹ç´ è§£æã§ããªã„ã¨å­¦ç¿’ã§ããªã„ã®ã§ã“ã“ã§çµ‚äº†
        raise SystemExit("GiNZA ãŒèª­ã¿è¾¼ã‚ãªã„ã®ã§çµ‚äº†ã—ã¾ã™ã€‚")

    # 1. CSVã‚’èª­ã‚€
    df = load_data(data_file)
    if df.empty:
        raise SystemExit("CSVãŒç©ºã ã£ãŸã®ã§çµ‚äº†ã—ã¾ã™ã€‚")

    # 2. TaggedDocumentã«ã™ã‚‹
    tagged_docs = to_tagged_docs(df)

    # 3. ãƒ¢ãƒ‡ãƒ«ã‚’ã¤ãã£ã¦ä¿å­˜ï¼ˆæ—¢å­˜ãŒã‚ã‚Œã°æ›´æ–°ï¼‰
    model = save_and_update_model(tagged_docs, BASE_DIR)
    print("ğŸ‰ å®Œäº†ã—ã¾ã—ãŸã€‚")
